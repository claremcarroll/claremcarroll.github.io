{
	"projects": [{
			"index": "bloomberg",
			"name": "MHCI Capstone + Bloomberg LP",
			"url": "",
			"video":"",
			"description":"",
			"cover": "img\/covers\/b7g.png",
			"tags":"CMU MHCI Capstone project with client Bloomberg LP",
			"previous":"",
			"next":"",
			"year":"2016 / 2017",
			"filters":"development emerging-tech ux-design interaction-design",
			"featured":"inline",
			"color":"#15486a",
			"client":"Bloomberg LP",
			"role":"Tech Lead",
			"duration":"9 Months",
			"brief":"This is my capstone project for Carnegie Mellon’s Masters in Human-Computer Interaction Program. For this project my team worked closely with Bloomberg to try to make financial visualizations more accessible to those who are visually impaired.<br><br> Click <a href=\"stockgrok.html\">here</a> For more information about our prototyping and development process.",
			"content1":"<div class=\"left\"><h1>The Problem Space</h1></div> <div class=\"right\"><p>Visual cues in charts are essential in financial decision-making. The sudden plummet of a stock price, the steady rise of an index fund, the outlier within a sector: these are all visual triggers that sighted finance experts regularly use to identify investment opportunities. <br><br>Because many financial data visualizations lack the necessary alternate text to be read by screen readers, people with visual impairments (PWVI) are at a huge disadvantage when it comes to making financial decisions as crucial information is being withheld. <br><br>For our masters' capstone project, we were asked to explore the domains of finance and visual accessibility, with the goal of giving PWVI better access to financial visualizations. </p></div>",
			"content2":"<div class=\"left\"><h1>Our Goals</h1></div> <div class=\"right\"><p>In our domain analysis, we saw that “people with visual impairments in finance” encompasses people across the spectrums of sight and prior financial knowledge. We scoped our focus to the following goal: <br><br><i>We want to create a user interface to help financially literate people who are blind confidently investigate market trends to identify ideal entry/ exit points for investment decisions. </i> </p></div>",
			"content3":"<div class=\"left\"><h1>Our Questions</h1></div><div class=\"right\"><p>Before we could design a user interface, we had to fully understand both the accessibility and financial visualizations domains. Some of our guiding questions for our research process were: <br><br><ol><li>How do people with visual impairments understand and use data?</li><li>What data and visualizations are most important to finance experts?</li><li> How do people across the sighted spectrum communicate with each other?</ol></p></div>",
			"content4":"<div class=\"left\"><h1>Research Process</h1></div><div class=\"right\"><p><b>Question Storming:</b> We worked with our client to create a list of questions to set the project scope.<br><br><b>Empathy Exercise:</b> After researching best practices in empathy exercises, we targeted our exercise to familiar finance tasks using a screen reader, creating a protocol designed to reduce prejudice and better understand the barriers that PWVI face. <br><br><b>Literature Review:</b> We read a large assortment of academic research papers on topics ranging from accessibility, vision loss, cognition, and data visualization.<br><br><b>Emerging Tech Survey:</b> We did an emerging technology survey to make sure we were apprised of new technologies that could help our users, such as haptic and tactile interfaces and 3D sound and data sonification experiments.<br><br><b>Early Prototyping:</b> We created early prototypes to explore the forms of potential solutions. We experimented with tactile materials and audio cues, testing internally with blindfold simulation to identify directions for higher fidelity prototyping and user testing.<br><br><b>Expert Interviews:</b> We conducted interviews with accessibility and emerging technology researchers as well as industry accessibility and finance technologists in order to get a better understanding of our problem space.<br><br><b>User Interviews:</b> We conducted six interviews with finance experts and 10 interviews with users who are legally blind, five from birth or childhood and five who lost their sight later in life.<br><br><b>Interpretation Sessions:</b> In order to synthesize findings from our user interviews, we interpreted our notes by modeling user workflows and identifying user breakdowns and attitudes. <br><br><b>Affinity Diagram:</b> From over 600 notes created in our interpretation sessions, we clustered notes into layers of logical groupings developed from the bottom up. Notes were color coded by the user type in order to see which clusters applied to both sighted finance experts and PWVI.<br><br><img class=\"nodrop\" src=\"../img/bloomberg/clare-secondary.jpg\"></p></div>",
			"content5":"<div class=\"left\"><h1>Research Insights</h1></div><div class=\"right\"><p><ol><li><b>People who help me are interfaces, too.</b><br>The transfer of information by a sighted individual is subject to that person’s interpretation and expertise. </li><li> <b>My AT can only provide one piece of info at a time.</b><br>It’s hard to view all parts of the puzzle at once because screen readers only provide one piece of information at a time. </li><li> <b>I use charts as a communication tool.</b><br>Highlighting visual cues in charts is the easiest way to persuade colleagues and clients of investment decisions. </li><li> <b>There is no normal.</b><br>Both PWVI and finance experts depend on their idiosyncratic workflows to get things done. </li><li>  <b>Changing tools is hard, so it better be worth it.</b><br>A product must offer a meaningful improvement to justify altering a hard-earned workflow. </li><li> <b>I use visuospatial metaphors as mnemonics.</b><br>Visual and spatial reference points help bridge communication between sighted and blind colleagues.</li></ol></p></div>",
			"content6":"<div class=\"left\"><h1>Opportunity Spaces</h1></div><div class=\"right\"><p>From our research and insights, we narrowed down to the following opportunity spaces: <br> <br> <p><b>Context for Visualizations</b></p><ul><li>Provide better descriptions of charts</li><li>Create a bridge between visuals and language</li></ul><br><p><b>Control in Data Navigation</b><p><ul><li>Provide PWVI the ability to “jump around”</li><li>Allow users to dive in deeper</li><li>Prototype multi-modal tools</li><ul><p><br><b>Confidence in Data Accuracy</b></p><ul><li>Ensure that data is laid out correctly</li><li>Equally useful for sighted financial experts</li></ul><p><br><b>Communication of Data</b></p><ul><li>Help sighted and PWVI communicate</li><li>Share data and visualizations</li></ul></p></div>",
			"content7":"<div class=\"left\"><h1>Product Design</h1></div><div class=\"right\"><p>Using our research, we began to rapidly generate as many ideas as possible in order to get a wide breadth of ideation. We then voted on the most promising ideas, and plotted them on a matrix that addressed the difficulty and impact of each idea. From this impact/difficulty matrix, we narrowed down to twelve ideas which we then rapidly prototyped and tested them with employees at Bloomberg.<br><br>From the results of our testing, we scoped down to one of our prototypes that allowed users to traverse through sonified price data, but in order to determine the best modality for this traversal we prototyped three different versions using a knob, keyboard, and drawing tablet. After user-testing these prototypes with people with visual impairments, we found that a keyboard system both made the most sense for the existing financial expert workflow and enabled users to navigate without purchasing any additional hardware. During this testing we also found that in order to allow our users to make meaningful financial decisions, we needed to provide more context to the price history.<br><br>In order to achieve this, we pivoted to sonifying the relationship between the price and a study or analysis over time. For the sake of our prototype and testing we used the 50 Day Simple Moving Average. We decided to build our solution as a web-based final product that it could be easily accessible on any operating system, and eventually across platforms (desktop, phone, tablet. etc.). We user-tested our final product with both people with visual impairment and sighted finance experts, and found that both were able to pick up on nonvisual cues about the changing trends in a security’s price history.</p></div>",
			"content8":"<div class=\"left\"><h1>Stockgrok</h1></div><div class=\"right\"><p>Our final product can be found <a href=\"https://stockgrok.github.io/prototype/\" target=\"_blank\" >here<br><br><img class=\"dropshadow\" src=\" ../img/bloomberg/stockgrok.png\"></a>.<br>Click <a href=\"stockgrok.html\">here</a> for more information about how we developed Stockgrok.</p></div>",
			"content9":"<div class=\"left\"><h1>Team</h1></div><div class=\"right\"><p><img src=\"../img/bloomberg/team.png\" class=\"nodrop\"><br><br>From left to right: Conrad Bassett-Bouchard, Emily Saltz, Nora Tane, Clare Carroll, and Jayanth Prathipati.</p></div>",
			"section":"",
			"rec1":"",
			"rec2":""
		},{
			"index": "robin",
			"name": "Robin",
			"url": "",
			"description":"",
			"cover": "img\/covers\/robin.png",
			"demo":"../robinprototype",
			"tags":"A conversational UI to enable independence for adults with dementia",
			"previous":"",
			"next":"",
			"year":"2016 / 2017",
			"filters":"ux-design interaction-design emerging-tech",
			"featured":"inline",
			"color":"#73a8ac",
			"role":"Prototyping and Visual Design Lead",
			"duration":"4 Months",
			"awards":"2nd Place, CHI Student Design Competition",
			"brief":"Robin is a prototype for a conversational UI that works with Amazon Alexa to provide assistance to users with cognitive impairments caused by conditions like dementia. This was my team's submission to the 2017 CHI Student Design Competition where we came in second place out of 70 entries from around the globe.",
			"content1":"<div class=\"left\"><h1>The Problem Space</h1></div> <div class=\"right\"><img class=\"nodrop\" src=\"../img/robin/problemspace.png\"></div>",
			"content2":"<div class=\"left\"><h1>Initial Research</h1></div><div class=\"right\"><p>We conducted interviews with caregivers and healthcare providers, and a review/analysis of dementia forums around the web. Through our research we identified some key insights: <br><br><ul><li><b>Varied Symptoms</b><Br>Individuals have extremely varied symptoms even though impairment caused by dementia progressed similarly.</li><br><li><b>Power of Routines</b><br>The further the disease progresses, the more individuals adhere to routines. Straying from these routines lead to negative health effects including increased hospitalization and mortality.</li> <Br><li><b>Elders want assistance</b><br>Elders are eager to adopt technologies that will help them maintain independence. 65% of forum respondents indicated that they would be interested in trying Robin. </li><br><li><b>Tech to help now</b><br>There are several cutting-edge interventions being explored and developed, but many are not market-ready and remain research projects.</li></ul></p></div>",
			"content3":"<div class=\"left\"><h1>Our Goal</h1></div><div class=\"right\"><p>From conversations with informal caregivers, we realized that the most impactful thing we can do to help caregivers and adults experiencing cognitive impairment is to use technology in ways that enable them to stay independent for as long as possible.</p></div>",
			"content4":"<div class=\"left\"><h1>Our Process</h1></div><div class=\"right\"><p>We used a dementia patient and a primary informal caregiver as user personas and created 30 scenarios. From this, we found three areas where routine assistance could help: reminders, step-by-step guidance, and improving quality-of-life.<br><br>We then storyboarded three scenarios and used them in collaborative speed-dating sessions with caregivers and healthcare providers. We determined from these speed-dating sessions that our solution should:<br><br><ul><li>Support Successful Routine Management</li><li>Enable users to remain independent in their homes for as long as possible</li><li>Be customizable and adaptable with the user’s cognitive decline</li><li>Be inexpensive and readily available</li></ul><br><br><p>We then created paper prototypes and performed four think-aloud usability tests. Using the feedback from these tests, we created a higher fidelity digital prototype of the application interface. We performed four additional think-alouds and continued to iteratively prototype. Finally, ee polished our solution by performing a heuristic evaluation using AARP-developed website heuristics for older adults.</p></div>",
			"content5":"<div class=\"left\"><h1>Our Solution</h1></div> <div class=\"right\"><p>We created Robin, a conversational user interface that evens the playing field by assisting older adults with dementia, or really anyone who needs help completing tasks in their home. Robin is a two part system: a conversational user interface and a tablet application.<br><br>For the conversational interface, users place Amazon dots throughout their home, starting with only the rooms where they are having difficulty completing tasks and adding devices over time as needed. This allows Robin to have contextual information about where the user is in the home, which in turn allows Robin to provide better assistance to the user.<br><br>Using the tablet application, users and their loved input tasks and routines, labeling them with a specific time and room. Rather than addressing very specific tasks and symptoms, Robin is a task-management platform that works with voice assistants like Amazon Alexa to provide three <i>types</i> of customizable services to users as needed within their own homes: helping you stay on routine, orienting you within your day to day activities, and guiding you through the granular steps of a specific task. As the you configure each routine item, you can set a threshold of when a reminder is appropriate for the item. This can be adjusted over time as you need more help remembering to perform certain tasks in your daily routine. It’s completely flexible based on the needs of the user.<br><br>Robin then becomes a partner in your daily routine, allowing users to ask questions for help and receive customized reminders throughout the day.</p><br><br><div class=\" video-responsive\"> <iframe src=\"https://player.vimeo.com/video/203604850\" width=\"100%\" width=\"640\" height=\"360\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe></div><br><br><a target=\"blank\" href=\"../robinprototype\">Prototype Demo</p><br><img style=\"border: 1px solid #cecece; \"class=\"nodrop\" src=\"../img/robin/prototype.png\"></a></div>",
			"content6":"<div class=\"left\"><h1>Why Robin?</h1></div> <div class=\"right\"><p>The use of routines helps users to regain confidence and control over their everyday lives. By alleviating the need for caregiver assistance in the early stages of cognitive decline, Robin allows users to stay self sufficient longer. This in turn allows for a decrease in dependence on loved ones and a reduction in the overwhelming amount of unpaid caregiver work.<br><br>By using Amazon assistant devices, Robin is something that could be on the market in the very near future. At only $50 for an Amazon Dot, users and their loved ones can easily buy these devices, install them, and begin integrating them into their everyday lives.<br><br>On top of this, we found that older-adults already use voice assistants in their day to day activities. In our validation survey after our design phase, one user explicitly said “I have been asking for someone to please make us a programmable Siri.” In addition, 65% of respondents said that they would be interested in trying a device like Robin.<br><br>Because many of Robin’s features are also applicable and useful to those not suffering from cognitive decline, we believe that Robin would be maintained and updated, contrary to many abandoned accessibility tools that were conceived without universal design in mind.<br><br>But most importantly, by offering context-appropriate assistance when the user needs it the most, we believe that Robin would enable users to stay independent for longer and enjoy a better quality of life.</p></div>",
			"content7":"<div class=\"left\"><h1>Poster</h1></div><div class=\"right\"><p><a target=\"_blank\" href=\"../img/robin/poster.jpg \"><img src=\"../img/robin/poster.jpg\"></a></p></div>",
			"content8":"<div class=\"left\"><h1>Paper</h1></div><div class=\"right\"><p><br><a target=\"_blank\" href=\"http://dl.acm.org/citation.cfm?id=3049266\"><img class=\"nodrop\" src=\" http://s3.amazonaws.com/libapps/customers/3442/images/ACMDL_Logo_Alt_Color.png\"></a><Br><Br></p></div>",
			"content9":"<div class=\"left\"><h1>Team</h1></div> <div class=\"right\"><img class=\"nodrop\"src=\"../img/robin/team.jpg\"><br><p>The team (from left to right): Jayanth Prathipati, Adena Lin, Meg Nidever, Clare Carroll, and Catherine Chiodo (not pictured).</p></div>",
			"section":"",
			"rec1":"",
			"rec2":""
		},{
			"index": "swivel",
			"name": "Swivel",
			"url": "https:\/\/www.behance.net\/gallery\/46479781\/Swivel",
			"video": "<iframe src=\"https://player.vimeo.com/video/196141320\" width=\"640\" height=\"360\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>",
			"description":"<p><br><br>For this project we were given 5 different kinds of technologies and were asked to pick one and design a product around it. We surveyed each piece of technology and brainstormed possible solutions. After this we determined the idea which we thought was most viable.<br><br>My team chose the connected scale technology and created a product called <i>Swivel<\/i>, an ergonomic office chair. Using the weight sensors in the seat and back, <i>Swivel<\/i> can determine when it is occupied, who is occupying it, and can provide real-time feedback regarding the occupant’s posture.<br><br>Swivel fulfills several needs in commercial office spaces, where poor ergonomics harmfully impact employee health and productivity. Additionally, it provides a rich set of data which employers can use to make their office space more intelligent and adaptive.<br><br>We created a concept video to demonstrate our idea.<\/p>",
			"cover": "img\/covers\/swivel.png",
			"client": "CMU MHCI",
			"tags":"Concept video for an ergonomic connected office chair.",
			"previous":"",
			"next":"",
			"year":"2016",
			"filters":"interaction-design ux-design project-management",
			"featured":"inline",
			"color":"#5d6172",
			"role":"Narrator and Motion Graphics Designer",
			"duration":"3 Weeks",
			"brief":"This is a concept video for Swivel, a connected smart chair. Swivel helps users to correct unhealthy postures and behaviors in the workplace while providing additional computer security and data about work place utilization.  ",
			"content1":"<div class=\"left\"><h1>The Problem</h1></div> <div class=\"right\"><p>Over 86% of office workers have experienced strain or soreness due to their desk setup or posture, resulting in injuries and illnesses that account for around 34% of lost workdays. For companies, this costs around 45 to 54 billion dollars of lost compensation, wages, and productivity every year.<br><br>Good ergonomic practices, especially proper sitting posture and desk setup, can prevent these musculoskeletal injuries. Is it possible that technology could help people gain better ergonomic practices by giving them real-time feedback on the healthiness of their posture?</p></div>",
			"content2":"<div class=\"left\"><h1>Our Solution</h1></div> <div class=\"right\"><p>Our concept is Swivel, a connected office chair that uses weight sensors in the seat and back. With these sensors, the chair can determine when it is occupied, know who is occupying it, and give the occupant real-time feedback on their posture.<br><br>We conceptualized Swivel as a solution not only for helping to improve employees’ health and productivity in the workplace, but also as a source of security and data for employers. Employers could use swivel as an extra authentication step for proprietary information and use data from Swivel to better understand how employees utilize the office space, helping them to make better decisions about office layout.</p></div>",
			"content3":"<div class=\"left\"><h1>Our Process</h1></div> <div class=\"right\"><p>We were given five technologies to explore: a camera ID with sensor, a connected scale, an inventory bag, micro location, and mixed reality. We started by rapidly generating ideas individually and then came together as a team to discuss the pros and cons of these ideas. <br><br>After brainstorming, we considered the tradeoffs of each idea and focused on ones that we thought had the most potential. We eventually narrowed down to two ideas: an inventory-sensing medical kit and a weight-sensing office chair. We selected the office chair because we thought it had wider potential applications and a richer set of data.<br><br>We identified businesses, particularly tech companies, as an ideal target customer for the weight sensing chair because they already invest heavily in ergonomic solutions for their employees.</p></div>",
			"content4":"<div class=\"left\"><h1>Competitive Value</h1></div> <div class=\"right\"><p>As we were conducting our research around ergonomics in the workplace, we encountered several products aimed towards addressing both the concerns of poor posture and infrequent breaks from sitting.<br><br>The major breakdown with these current devices is that <b>they are not always accurate or convenient to use</b>. Because Swivel is embedded in the chair, it can more accurately tell whether it is occupied or not without requiring its occupant to remember any additional devices.<br><br>Another opportunity to make the connected chair stand more valuable than its competition is the potential for data. If all of the chairs in an office are able to track when they were occupied, this could allow companies to get a clear picture of how employees utilize their office space. This is increasingly important for companies as they must make decisions about how to restructure office spaces or identify areas they can repurpose or eliminated.<br><br>In addition to providing information on space utilization, we also realized that the chair had potential security applications as well. If it could learn who was sitting on it, it could load up an occupant’s personal desktop as soon as they sit down. Additionally, the chair could lock the computer screen after becoming unoccupied, which would reduce the security risk of unattended workstations.</p></div>",
			"content5":"<div class=\"left\"><h1>Team</h1></div> <div class=\"right\"><p>Bo Kim and Shannon Sullivan</p></div>",
			"section":"",
			"rec1":"",
			"rec2":""
		},{
			"index": "blue",
			"name": "Body Labs Blue",
			"url": "https:\/\/www.behance.net\/gallery\/41917453\/Body-Labs-Blue",
			"video":"<video controls width=\"100%\" poster=\"../img/bluecover.png\"><source src=\"../video/blueproductvideo.mp4\" type=\"video/mp4\"></video>",
			"description": "<br><br><p>Although we want users to trust our predictive algorithm, we also want to enable them to add more specific information accurately. <br><br><center><a target=\"_blank\" href=\"https:\/\/www.bodylabs.com\/solutions\/spectrum\/blue\/\">View Blue Here<\/a><\/center><br><br><img class=\"fullw\" src=\"http:\/\/claremarie.info\/img\/portfolio\/blscreenshot.png\"><\/p>",
			"cover": "img\/covers\/blue.png",
			"tags":"Interface for creating custom clothing from measurements.",
			"previous":"",
			"next":"",
			"year":"2016",
			"filters":"ux-design interaction-design",
			"featured":"none",
			"color":"#28b2c9",
			"company":"Body Labs",
			"role":"UX/UI Designer and Developer",
			"duration":"4 Months",
			"brief":"This is a project I lead as the sole UX Designer at Body Labs. Body Labs Blue is a tool for creating custom clothing through four key measurements. The goal of this product is to allow users to feel confident buying custom clothing when entering only a few measurements but also to allow users to manually enter correct measurements as desired.<br><br>Body Labs is now a part of Amazon.",
			"content1":"<div class=\"left\"><h1>Product Description</h1></div> <div class=\"right\"><p>Through an easily embedded Web interface, Body Labs Blue uses artificial intelligence (AI) and machine learning algorithms to reference just the height and weight of your customers to predict 19 highly accurate additional measurements ideal for custom clothing or sizing recommendations. To help ensure accuracy, your customers can also refine six-key measurements predicted by Blue. This enables your business to deliver clothing customization and standardization that scales for any purchase volume or web traffic.</p></div>",
			"content2":"<div class=\"left\"><h1>Product Demo</h1></div> <div class=\"right\"><p>In this demo, Jon enters his 4 key measurements and Blue calculates his most likely measurements. He can then either shop immediately, or refine his results. Jon is then shown how each measurement is accurately taken and his predicted result. After Jon enters the measurements he feels are necessary, he can hit update to get a new set of predicted measurements.<br><br><video width=\"100%\" controls ><source src=\"../video/blue.mp4\" type=\"video/mp4\"></p></div>",
			"content3":"<div class=\"left\"><h1>User Testing Insights</h1></div> <div class=\"right\"><p><ol><li><b>Users didn’t believe the calculation could happen instantaneously</b><br>In our original tests, users got their results immediately after hitting the submit button with their initial measurements. Because this was so fast, users didn’t trust the calculation. To fix this we added a loading screen.</li><li><b>Measurement names are nonstandard</b><br>The term \"waist\" means something different to every person. We had to make sure our users knew exactly what we meant by each term so that they could take correct measurements.</li><li><b>Users don’t know how to take their measurements</b><br>In addition to the terms being confusing, users often took their measurements sitting down at their computer, resulting in inaccurate measurements. We had to make sure to provide clear instructions about how to take each measurement so users could provide accurate data.</li><li><b>People don’t want to see their body models</b><br>In many of our early prototypes, we showed the body model created from the measurements provided. We found that people didn’t identify with their body model and were less likely to believe their results. In the final version, we removed the body model, only showing a model for measurement instructions</li></ol></p></div>",
			"content4":"<div class=\"left\"><h1>Axure Prototypes</h1></div> <div class=\"right\"><p>I did most of the prototyping for this product in Axure. Here are some of my earlier prototypes:<br><Br><img class=\"\" src=\"../img/blue/gif1.gif\"> <br><Br><img class=\" \" src=\"../img/blue/gif2.gif\"><br><Br><img class=\" \" src=\"../img/blue/gif3.gif\"><br><Br><img class=\" \" src=\"../img/blue/gif4.gif\"></p></div>",
			"section":"",
			"rec1":"",
			"rec2":""
		},{
			"index": "holojam",
			"name": "Holojam",
			"url": "https:\/\/www.behance.net\/gallery\/35388195\/Holojam",
			"video":"<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/kzx5igORwk4\" frameborder=\"0\" allowfullscreen></iframe>",
			"description": "<p><br><center><a href=\"http:\/\/mrlholojam.tumblr.com\">Official Website<\/a><\/center>",
			"cover": "img\/covers\/holojam.png",
			"tags":"Multi-user VR drawing experience.",
			"previous":"",
			"next":"",
			"year":"2015",
			"filters":"project-management ux-design emerging-tech",
			"featured":"none",
			"color":"#9974af",
			"role":"User Experience Designer",
			"group":"NYU Media Research Lab",
			"demos":"SIGGRAPH 2015",
			"brief":"Holojam is an experience created by my team at the NYU Media Research Lab under the direction of Ken Perlin. We created an experience where up to five people can draw collaboratively in virtual reality. This version of Holojam was showcased at the SIGGRAPH 2015 VR Village.",
			"content1":"<div class=\"left\"><h1>About the Experience</h1></div> <div class=\"right\"><p>Up to five people in a 36ft x 34ft physical space can run around freely, untethered, seeing each other as avatars.  Each person wears a headset, wrist straps and ankle straps (it takes about 30 seconds to put these on).  All info needed by the software to build a full human avatar comes from these five strapped on markers. In addition, participants will hold wands, which they can use to paint and sculpt in the 3D space.<br><br>Unlike much shared VR, this is a highly physical experience.  Participants see each other in their true physical locations, re-created as avatars in an alternate “magical” world.  They are free to talk to each other and physically interact with each other, while they walk and run around in the shared space.  With their magic wand they can draw sculptural shapes in the air.  In this way, each participant contributes to an on-going three dimensional sculptural art work, which is collective created by all participants over the course of the day.<br><br>The participating artists are able to choose different kinds of brushes and colors to paint with. In some versions of Holojam, these may be chosen with palettes placed around the room, or by touching exotic colorful textured creatures that swim/float/fly by in the air.  A participant who runs up to one of these creatures before it leaves the space can touch the creature with his/her wand, and will then be able to draw in the air with that color and texture.<br><br>Over the course of a day, participants collectively create a cadavre exquis, in the form of an evolving space-time sculpture.  At any given moment, participants see only a time-slice of this sculpture.  After a few minutes, earlier portions of the sculpture appear to fade away, as those portions recede into the past.</p></div>",
			"content2":"<div class=\"left\"><h1>WHY THE EXPERIENCE IS UNIQUE</h1></div><div class=\"right\"><p>Our shared VR experience is different from all previous shared VR experiences in that participants can run around freely, unconstrained by cables or bulky backpacks, interacting with other participants in a magical, people-centered way, doing impossible things together like drawing in the air.  It is designed to be a highly social experience for participants, and also lots of fun for other people watching.<br><br>Participants can talk to, observe, and physically interact with one and other in the space. More participants provides more activity to observe.<br><br>People who experienced Holojam at SIGGRAPH 2015 created a large and spectacular shared drawing, using their own physical movement.  This is a unique example of community-created art in a shared VR world – the first instance in history, we believe, of a work of community-created space-time art in shared virtual reality.</p></div>",
			"content3":"<div class=\"left\"><h1>Process and Ideation</h1></div><div class=\"right\"><p>Holojam was built collaboratively with a large team of brilliant developers, designers, and animators. The final product is a result of freeform research and ideation. Here are some of the design ideas I came up for our initial implementation.<br><br><div class=\"sketchfab-embed-wrapper\"><iframe width=\"100%\" height=\"300\" src=\"https://sketchfab.com/models/1684c84a8c98458499ec49c750bdcc17/embed\" frameborder=\"0\" allowvr allowfullscreen mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\" onmousewheel=\"\"></iframe></div><br><br><div class=\"sketchfab-embed-wrapper\"><iframe width=\"100%\" height=\"300\" src=\"https://sketchfab.com/models/e7ca705bb70e48618d15b956b86b6c78/embed\" frameborder=\"0\" allowvr allowfullscreen mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\" onmousewheel=\"\"></iframe></div></p></div>",
			"content4":"<div class=\"left\"><h1>SIGGRAPH VR Village</h1></div><div class=\"right\"><p><div class=\" video-responsive\"><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/zB9Vw1vPo20\" frameborder=\"0\" allowfullscreen></iframe></div></p></div>",
			"content5":"<div class=\"left\"><h1>Team</h1></div> <div class=\"right\"><p>Connor Defanti, Zach Cimafonte, Laura Juo-Hsin Chen, Sebastian Herscher, Will Field-Thompson, Daren Liu, David Lobser, Ken Perlin.</p></div>",
			"content6":"<div class=\"left\"><h1>Press</h1></div> <div class=\"right\"><p>Motherboard, MIT Technology Review, UploadVR, WSN, VRR</p></div>",
			"section":"",
			"rec1":"",
			"rec2":""
		},{
			"index": "",
			"name": "",
			"url": "",
			"video":"",
			"description": "",
			"cover": "",
			"tags":"",
			"previous":"",
			"next":"",
			"year":"",
			"filters":"",
			"featured":"",
			"color":"",
			"role":"",
			"group":"",
			"demos":"",
			"brief":"",
			"content1":"",
			"section":"",
			"rec1":"",
			"rec2":""
		}

	]

}